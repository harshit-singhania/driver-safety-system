{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNseF0sOAGCU"
   },
   "source": [
    "# Driver Alert System Prototype for Smart India Hackathon\n",
    "\n",
    "This Jupyter Notebook contains a prototype version of our Driver Safety system for the Smart India Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drowsiness Detection using Mediapipe in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the prototype version of the final Driver Safety System. This prototype version contains the drowsiness detection system using Mediapipe in Python. The final version will contain the drowsiness detection system, the distraction detection system and the alert system along with all other functionalities. \n",
    "\n",
    "As far as the prototype is concerned, we need 3 things to create a drowsiness detection,' system. \n",
    "\n",
    "1. `Face Detection` - To detect the face in the frame. We can use any camera for this purpose.\n",
    "2. `Facial Landmark Detection` - To detect the facial landmarks in the face detected. We will use Mediapipe for this purpose. We use the pre built ``MediaPipe Face Detection`` and ``MediaPipe Face Mesh`` models for this purpose.\n",
    "3. `Drowsiness Detection` - To detect the drowsiness of the driver. We will use the EAR (Eye Aspect Ratio) for this purpose. This approach has been detailed in the simple yet robust research paper Real-Time Eye Blink Detection using Facial Landmarks\n",
    "(https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf) by Soukupová and Čech in 2016. In the above-linked paper, the authors have described their approach to blink detection. An eye blink is a speedy closing and reopening action. For this, the authors use an SVM classifier to detect eye blink as a pattern of EAR values in a short temporal window. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect drowsiness, all we have to keep in mind is that **our eyes close when we feel drowsy** and hence we can use the EAR to detect drowsiness. \n",
    "\n",
    "The EAR is calculated as follows: \n",
    "\n",
    "<img src=\"images\\04-driver-drowsiness-detection-EAR-equation-1-768x167.png\"> \n",
    "\n",
    "The EAR formula returns a single scalar quantity that reflects the level of eye-opening.\n",
    "\n",
    "\n",
    "<img src=\"images\\03-driver-drowsiness-detection-EAR-points-768x297.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our approach works as follows- \n",
    "\n",
    "1. First, we declare two threshold values and a counter.\n",
    "    1. EAR_thresh: A threshold value to check whether the current EAR value is within range.\n",
    "    2. D_TIME: A counter variable to track the amount of time passed with current EAR < EAR_THRESH. \n",
    "    3. WAIT_TIME: To determine whether the amount of time passed with EAR < EAR_THRESH exceeds the permissible limit. \n",
    "2. When the application starts, we record the current time (in seconds) in a variable t1 and read the incoming frame.\n",
    "3. Next, we preprocess and pass the frame through Mediapipe’s Face Mesh solution pipeline. \n",
    "4. We retrieve the relevant (Pi) eye landmarks if any landmark detections are available. Otherwise, reset t1 and D_TIME (D_TIME is also reset over here to make the algorithm consistent).\n",
    "5. If detections are available, calculate the average EAR value for both eyes using the retrieved eye landmarks.\n",
    "6. If the current EAR < EAR_THRESH, add the difference between the current time t2 and t1 to D_TIME. Then reset t1 for the next frame as t2.\n",
    "7. If the D_TIME >= WAIT_TIME, we sound the alarm or move on to the next frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmark Detection Using Mediapipe Face Mesh In Python\n",
    "\n",
    "We will use the pre built ``MediaPipe Face Detection`` and ``MediaPipe Face Mesh`` models for this purpose.\n",
    "\n",
    "Since we are focusing on driver drowsiness detection, out of the 468 points, we only need landmark points belonging to the eye regions. The eye regions have 32 landmark points (16 points each). For calculating the EAR, we require only 12 points (6 for each eye).\n",
    "\n",
    "the model first utilizes face detection along with a facial landmark detection model. For face detection, the pipeline uses the BlazeFace model, which has a very high inference speed. The BlazeFace model is a lightweight model that is optimized for mobile devices. It is based on the Single Shot Detector (SSD) framework with a custom lightweight backbone network. The model is trained using the Quantization Aware Training (QAT) technique to reduce the model size and inference latency. The model is trained on the WIDERFACE dataset, which contains 32,203 images and 393,703 faces with a high degree of variability in scale, pose, and occlusion as depicted in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Eye Aspect Ratio (EAR) Technique\n",
    "\n",
    "1. We will use Mediapipe’s Face Mesh solution to detect and retrieve the relevant landmarks in the eye region (points P1 – P6 in the below image).\n",
    "2. After retrieving the relevant points, the Eye Aspect Ratio (EAR) is computed between the height and width of the eye.\n",
    "\n",
    "The EAR is mostly constant when an eye is open and gets close to zero, while closing an eye is partially person, and head pose insensitive. The aspect ratio of the open eye has a small variance among individuals. It is fully invariant to a uniform scaling of the image and in-plane rotation of the face. Since eye blinking is performed by both eyes synchronously, the EAR of both eyes is averaged. \n",
    "\n",
    "First, we have to calculate Eye Aspect Ratio for each eye:\n",
    "\n",
    "<img src=\"images\\11-driver-drowsiness-detection-EAR-equation-left-right-740x340.webp\">\n",
    "\n",
    "For calculating the final EAR value, the authors suggest averaging the two EAR values.\n",
    "\n",
    "<img src=\"images\\12-driver-drowsiness-detection-AVG_EAR-equation-768x193.webp\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a decent idea of how our drowsiness detection works, let’s move on to the implementation part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementatation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to import the necessary libraries. \n",
    "\n",
    "1. `cv2` - OpenCV is a library of programming functions mainly aimed at real-time computer vision. We will use OpenCV to read the video stream from the webcam.\n",
    "2. `numpy` - NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. We will use NumPy to perform numerical computations.\n",
    "3. `matplotlib` - Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. We will use Matplotlib to plot the graphs.\n",
    "4. `mediapipe` - MediaPipe offers ready-to-use yet customizable Python solutions as a prebuilt Python package. We will use MediaPipe to detect the facial landmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to install the above libraries, run the following command in your terminal:\n",
    "\n",
    "```pip install -r requirements.txt```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.167797Z",
     "start_time": "2022-09-22T17:23:25.136805Z"
    },
    "id": "Y0z9ncaRAGCn"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_facemesh = mp.solutions.face_mesh\n",
    "mp_drawing  = mp.solutions.drawing_utils\n",
    "denormalize_coordinates = mp_drawing._normalized_to_pixel_coordinates\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "718zRd1TAGCn"
   },
   "source": [
    "The next thing that needs to be done is to calculate the landmark points for the eyes. We will use the pre built ``MediaPipe Face Detection`` and ``MediaPipe Face Mesh`` models for this purpose.\n",
    "\n",
    "Since we are focusing on driver drowsiness detection, out of the 468 points, we only need landmark points belonging to the eye regions. The eye regions have 32 landmark points (16 points each). For calculating the EAR, we require only 12 points (6 for each eye).\n",
    "\n",
    "In the following code snippet, we are working with MediaPipe's FaceMesh in order to extract the eye landmarks for the left and the right eyes. The code provides combined landmark points in order for us to plot them on the face. \n",
    "\n",
    "1. ``mp_facemesh.FACEMESH_LEFT_EYE`` - This is the list of landmark points for the left eye.\n",
    "2. ``mp_facemesh.FACEMESH_RIGHT_EYE`` - This is the list of landmark points for the right eye. \n",
    "3. ``all_left_eye_idxs`` and ``all_right_eye_idxs`` are created by converting these lists into regular Python lists and then flattening them using ``np.ravel``. (Flattening essentially converts a nested list into a single-dimensional list and removes duplicates.) \n",
    "4. The ``set()`` function is used to remove any duplicate landmarks that may have been present in the flattened lists. This ensures that each landmark point is unique within its respective eye.\n",
    "5. Finally, ``all_idxs`` is created by taking the union of ``all_left_eye_idxs`` and ``all_right_eye_idxs``. This combines all the unique landmark points from both the left and right eyes into a single set for plotting purposes.\n",
    "\n",
    "After running this code, ``all_left_eye_idxs`` and ``all_right_eye_idxs`` will contain the unique landmark points for the left and right eyes, respectively, and all_idxs will contain all the unique landmark points for both eyes combined, which you can use for visualization or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.183791Z",
     "start_time": "2022-09-22T17:23:26.170798Z"
    },
    "id": "GgBDNvnrAGCo",
    "outputId": "ae8c4e9b-e059-4f3a-c3c9-5bcf0eccd2ad"
   },
   "outputs": [],
   "source": [
    "# Landmark points corresponding to left eye\n",
    "all_left_eye_idxs = list(mp_facemesh.FACEMESH_LEFT_EYE)\n",
    "all_left_eye_idxs = set(np.ravel(all_left_eye_idxs)) # flatten and remove duplicates\n",
    "print(\"Left eye landmarks:\", all_left_eye_idxs)\n",
    "\n",
    "# Landmark points corresponding to right eye\n",
    "all_right_eye_idxs = list(mp_facemesh.FACEMESH_RIGHT_EYE)\n",
    "all_right_eye_idxs = set(np.ravel(all_right_eye_idxs)) # flatten and remove duplicates\n",
    "print(\"Right eye landmarks:\", all_right_eye_idxs)\n",
    "\n",
    "# Combined for plotting use - Landmark points for both eye\n",
    "all_idxs = all_left_eye_idxs.union(all_right_eye_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.198799Z",
     "start_time": "2022-09-22T17:23:26.187800Z"
    },
    "id": "-X0TqNd0AGCp"
   },
   "outputs": [],
   "source": [
    "# The chosen 12 points:   P1,  P2,  P3,  P4,  P5,  P6\n",
    "chosen_left_eye_idxs  = [362, 385, 387, 263, 373, 380]\n",
    "chosen_right_eye_idxs = [33,  160, 158, 133, 153, 144]\n",
    "\n",
    "all_chosen_idxs = chosen_left_eye_idxs + chosen_right_eye_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to load the images and videos. We will use the OpenCV library for this purpose. \n",
    "\n",
    "1. `cv2.imread()` - This function loads an image from the specified file.\n",
    "2. `cv2.cvtColor()` - This function converts an image from one color space to another. We will use this function to convert the image from BGR to RGB color space.\n",
    "3. `np.ascontigousarray()` - This function creates a contiguous array (data is stored in a single continuous block of memory) from the given array (non-contiguous array). We will use this function to convert the image to a contiguous array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.401791Z",
     "start_time": "2022-09-22T17:23:26.200795Z"
    },
    "code_folding": [],
    "id": "GEvNPrSbAGCp",
    "outputId": "91e8f331-8475-4b7f-962b-28ca8f340690"
   },
   "outputs": [],
   "source": [
    "# Load an image. \n",
    "\n",
    "image = cv2.imread(\"test-open-eyes.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB\n",
    "\n",
    "image = np.ascontiguousarray(image)\n",
    "\n",
    "imgH, imgW, _ = image.shape\n",
    "\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `mp_face_mesh` module contains the MediaPipe Face Mesh solution. To use this module, we need to first create a `FaceMesh` object. We can then use this object to process the incoming frames. \n",
    "\n",
    "The `FaceMesh` object can be created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.447789Z",
     "start_time": "2022-09-22T17:23:26.403796Z"
    },
    "id": "nzk1sU9IAGCq",
    "outputId": "d1a0d7d3-f540-479c-d41a-b96ee46e77a0"
   },
   "outputs": [],
   "source": [
    "# Running inference using static_image_mode \n",
    "\n",
    "with mp_facemesh.FaceMesh(\n",
    "    static_image_mode=True,        # Default=False\n",
    "    max_num_faces=1,               # Default=1\n",
    "    refine_landmarks=False,        # Default=False\n",
    "    min_detection_confidence=0.5,  # Default=0.5\n",
    "    min_tracking_confidence= 0.5,  # Default=0.5\n",
    ") as face_mesh:\n",
    "    \n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "print(bool(results.multi_face_landmarks)) # Indicates whether any detections are available or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ``landmark_0`` is assigned the coordinates ``(X, Y, Z)`` of the first facial landmark (index 0) from the detected face landmarks of the first face in the frame. This assumes that results.multi_face_landmarks is a list of face landmarks, and you're working with the first face in the list.\n",
    "\n",
    "2. ``landmark_0_x`` is calculated as the X-coordinate of ``landmark_0``, scaled by the width of the image ``(imgW)``.\n",
    "\n",
    "3. ``landmark_0_y`` is calculated as the Y-coordinate of ``landmark_0``, scaled by the height of the image ``(imgH)``.\n",
    "\n",
    "4. ``landmark_0_z`` is calculated as the Z-coordinate of ``landmark_0``, scaled by the width of the image ``(imgW)``. The documentation indicates that Z-coordinates are typically scaled by the width of the image.\n",
    "\n",
    "5. The X, Y, and Z coordinates of landmark_0 are printed.\n",
    "\n",
    "6. An empty line is printed for formatting.\n",
    "\n",
    "7. The total number of ``landmarks`` in ``results.multi_face_landmarks[0].landmark`` and is printed.\n",
    "\n",
    "Based on the code and its output, you are extracting the coordinates of the first facial ``landmark`` (landmark_0) and scaling them to match the dimensions of the original image. The X and Y coordinates are scaled by the width and height of the image, respectively, while the Z coordinate is scaled by the width of the image as per the documentation.\n",
    "\n",
    "The last print statement indicates the total number of landmarks available for the first detected face, which helps you understand how many landmarks are associated with each face in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.463794Z",
     "start_time": "2022-09-22T17:23:26.450793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the first landmark point on the first detected face\n",
    "\n",
    "landmark_0 = results.multi_face_landmarks[0].landmark[0]\n",
    "print(landmark_0)\n",
    "\n",
    "landmark_0_x = landmark_0.x * imgW\n",
    "landmark_0_y = landmark_0.y * imgH\n",
    "landmark_0_z = landmark_0.z * imgW # according to documentation\n",
    "\n",
    "print(\"X:\", landmark_0_x)\n",
    "print(\"Y:\", landmark_0_y)\n",
    "print(\"Z:\", landmark_0_z)\n",
    "\n",
    "print()\n",
    "print(\"Total Length of '.landmark':\", len(results.multi_face_landmarks[0].landmark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below defines a function named `plot` that visualizes facial landmarks detected by a model like MediaPipe's FaceMesh. Here's a breakdown of what the code does:\n",
    "\n",
    "1. **Function Arguments**: The function takes several arguments, all of which are optional and have default values. These include images (`img_dt`, `img_eye_lmks`, `img_eye_lmks_chosen`), face landmarks (`face_landmarks`), and parameters for drawing (`ts_thickness`, `ts_circle_radius`, `lmk_circle_radius`, `name`).\n",
    "\n",
    "2. **Drawing Utilities Initialization**: The function initializes drawing utilities for plotting the face mesh tessellation.\n",
    "\n",
    "3. **Landmarks Plotting**: It iterates over all landmarks and plots circles at the coordinates of each landmark. It differentiates between all eye landmarks and chosen landmarks.\n",
    "\n",
    "4. **Image Plotting**: It plots three images: one with the face mesh tessellation, one with all eye landmarks, and one with chosen landmarks. These are displayed as subplots in a matplotlib figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.479800Z",
     "start_time": "2022-09-22T17:23:26.465799Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot(\n",
    "    *,\n",
    "    img_dt,\n",
    "    img_eye_lmks=None,\n",
    "    img_eye_lmks_chosen=None,\n",
    "    face_landmarks=None,\n",
    "    ts_thickness=1,\n",
    "    ts_circle_radius=2,\n",
    "    lmk_circle_radius=3,\n",
    "    name=\"1\",\n",
    "):\n",
    "    # For plotting Face Tessellation\n",
    "    image_drawing_tool = img_dt \n",
    "    \n",
    "     # For plotting all eye landmarks\n",
    "    image_eye_lmks = img_dt.copy() if img_eye_lmks is None else img_eye_lmks\n",
    "    \n",
    "    # For plotting chosen eye landmarks\n",
    "    img_eye_lmks_chosen = img_dt.copy() if img_eye_lmks_chosen is None else img_eye_lmks_chosen\n",
    "\n",
    "    # Initializing drawing utilities for plotting face mesh tessellation\n",
    "    connections_drawing_spec = mp_drawing.DrawingSpec(\n",
    "        thickness=ts_thickness, circle_radius=ts_circle_radius, color=(255, 255, 255)\n",
    "    )\n",
    "\n",
    "    # Initialize a matplotlib figure.\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    fig.set_facecolor(\"white\")\n",
    "\n",
    "    # Draw landmarks on face using the drawing utilities.\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=image_drawing_tool,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_facemesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=connections_drawing_spec,\n",
    "    )\n",
    "\n",
    "    # Get the object which holds the x, y and z coordinates for each landmark\n",
    "    landmarks = face_landmarks.landmark\n",
    "\n",
    "    # Iterate over all landmarks.\n",
    "    # If the landmark_idx is present in either all_idxs or all_chosen_idxs,\n",
    "    # get the denormalized coordinates and plot circles at those coordinates.\n",
    "\n",
    "    for landmark_idx, landmark in enumerate(landmarks):\n",
    "        if landmark_idx in all_idxs:\n",
    "            pred_cord = denormalize_coordinates(landmark.x, landmark.y, imgW, imgH)\n",
    "            cv2.circle(image_eye_lmks, pred_cord, lmk_circle_radius, (255, 255, 255), -1)\n",
    "\n",
    "        if landmark_idx in all_chosen_idxs:\n",
    "            pred_cord = denormalize_coordinates(landmark.x, landmark.y, imgW, imgH)\n",
    "            cv2.circle(img_eye_lmks_chosen, pred_cord, lmk_circle_radius, (255, 255, 255), -1)\n",
    "\n",
    "\n",
    "    # Plot post-processed images\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Face Mesh Tessellation\", fontsize=18)\n",
    "    plt.imshow(image_drawing_tool)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"All eye landmarks\", fontsize=18)\n",
    "    plt.imshow(image_eye_lmks)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_eye_lmks_chosen)\n",
    "    plt.title(\"Chosen landmarks\", fontsize=18)\n",
    "    plt.axis(\"off\")\n",
    "#     plt.subplots_adjust(left=0.02, right=0.98, top=None, bottom=0.4, hspace=1.0)\n",
    "#     plt.savefig(f'image_{name}.png', dpi=200.0, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet below is checking if there are any face landmarks detected in the image. If there are (`if results.multi_face_landmarks:`), it iterates over each detected face. For each face, it calls the `plot` function to visualize the facial landmarks on a copy of the original image.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "1. `if results.multi_face_landmarks:`: This line checks if any face landmarks have been detected in the image.\n",
    "\n",
    "2. `for face_id, face_landmarks in enumerate(results.multi_face_landmarks):`: This line starts a loop that iterates over each detected face. For each face, it provides an ID (`face_id`) and the facial landmarks (`face_landmarks`).\n",
    "\n",
    "3. `_ = plot(img_dt=image.copy(), face_landmarks=face_landmarks)`: This line calls the `plot` function, which visualizes the facial landmarks on a copy of the original image. The function's return value is not stored (hence the `_`), indicating that we're only interested in the side effect of the function (i.e., the visualization) and not its return value.\n",
    "\n",
    "Please note that this code assumes that `results` is a variable containing the output of a facial landmark detection model, and `image` is the image on which detections were performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.883803Z",
     "start_time": "2022-09-22T17:23:26.481803Z"
    }
   },
   "outputs": [],
   "source": [
    "# If detections are available.\n",
    "if results.multi_face_landmarks:\n",
    "    \n",
    "    # Iterate over detections of each face. Here, we have max_num_faces=1, \n",
    "    # so there will be at most 1 element in the 'results.multi_face_landmarks' list            \n",
    "    # Only one iteration is performed.\n",
    "\n",
    "    for face_id, face_landmarks in enumerate(results.multi_face_landmarks):    \n",
    "        _ = plot(img_dt=image.copy(), face_landmarks=face_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.899795Z",
     "start_time": "2022-09-22T17:23:26.885799Z"
    },
    "id": "DlQw3zMyAGCv"
   },
   "outputs": [],
   "source": [
    "def distance(point_1, point_2):\n",
    "    \"\"\"Calculate l2-norm between two points\"\"\"\n",
    "    dist = sum([(i - j) ** 2 for i, j in zip(point_1, point_2)]) ** 0.5\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_ear` calculates the Eye Aspect Ratio (EAR) for one eye based on facial landmarks. Here's a breakdown of what it does:\n",
    "\n",
    "1. **Function Arguments**: The function takes several arguments:\n",
    "   - `landmarks`: A list of detected landmarks.\n",
    "   - `refer_idxs`: Index positions of the chosen landmarks in order P1, P2, P3, P4, P5, P6.\n",
    "   - `frame_width` and `frame_height`: The dimensions of the captured frame.\n",
    "\n",
    "2. **Coordinate Calculation**: For each landmark index in `refer_idxs`, it denormalizes the coordinates and appends them to `coords_points`.\n",
    "\n",
    "3. **Distance Calculation**: It calculates the Euclidean distance between pairs of points (P2-P6, P3-P5, and P1-P4).\n",
    "\n",
    "4. **EAR Calculation**: It computes the Eye Aspect Ratio (EAR) using the formula `(P2_P6 + P3_P5) / (2.0 * P1_P4)`.\n",
    "\n",
    "5. **Error Handling**: If an error occurs during these calculations (for example, if a landmark is not detected), it sets `ear` to 0.0 and `coords_points` to None.\n",
    "\n",
    "6. **Return Values**: It returns the calculated EAR and the list of coordinates.\n",
    "\n",
    "This function can be used in applications like drowsiness detection, where a decrease in EAR can indicate that a person is starting to fall asleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.915813Z",
     "start_time": "2022-09-22T17:23:26.902792Z"
    },
    "code_folding": [],
    "id": "Uu5JlzCwAGCw"
   },
   "outputs": [],
   "source": [
    "def get_ear(landmarks, refer_idxs, frame_width, frame_height):\n",
    "    \"\"\"\n",
    "    Calculate Eye Aspect Ratio for one eye.\n",
    "\n",
    "    Args:\n",
    "        landmarks: (list) Detected landmarks list\n",
    "        refer_idxs: (list) Index positions of the chosen landmarks\n",
    "                            in order P1, P2, P3, P4, P5, P6\n",
    "        frame_width: (int) Width of captured frame\n",
    "        frame_height: (int) Height of captured frame\n",
    "\n",
    "    Returns:\n",
    "        ear: (float) Eye aspect ratio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Compute the euclidean distance between the horizontal\n",
    "        coords_points = []\n",
    "        for i in refer_idxs:\n",
    "            lm = landmarks[i]\n",
    "            coord = denormalize_coordinates(lm.x, lm.y, frame_width, frame_height)\n",
    "            coords_points.append(coord)\n",
    "\n",
    "        # Eye landmark (x, y)-coordinates\n",
    "        P2_P6 = distance(coords_points[1], coords_points[5])\n",
    "        P3_P5 = distance(coords_points[2], coords_points[4])\n",
    "        P1_P4 = distance(coords_points[0], coords_points[3])\n",
    "\n",
    "        # Compute the eye aspect ratio\n",
    "        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)\n",
    "\n",
    "    except:\n",
    "        ear = 0.0\n",
    "        coords_points = None\n",
    "\n",
    "    return ear, coords_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcYE6OmqAGCw"
   },
   "source": [
    "Calculate `EAR` for the previously detected landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.931808Z",
     "start_time": "2022-09-22T17:23:26.917804Z"
    },
    "id": "iZPPIclnAGCw"
   },
   "outputs": [],
   "source": [
    "landmarks = face_landmarks.landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.947803Z",
     "start_time": "2022-09-22T17:23:26.936805Z"
    },
    "id": "SOHiFWpGAGCw",
    "outputId": "70afced5-b1a7-40fa-d60d-e5c5cf3ddd7b"
   },
   "outputs": [],
   "source": [
    "left_ear, _  = get_ear(landmarks, chosen_left_eye_idxs, imgW, imgH)\n",
    "right_ear, _ = get_ear(landmarks, chosen_right_eye_idxs, imgW, imgH)\n",
    "\n",
    "EAR = (left_ear + right_ear) / 2\n",
    "\n",
    "print(\"left_ear: \", left_ear)\n",
    "print(\"right_ear:\", right_ear)\n",
    "print(\"Avg. EAR: \", EAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:26.963808Z",
     "start_time": "2022-09-22T17:23:26.949792Z"
    },
    "code_folding": [],
    "id": "WxNSt1r5AGCx"
   },
   "outputs": [],
   "source": [
    "def calculate_avg_ear(landmarks, left_eye_idxs, right_eye_idxs, image_w, image_h):\n",
    "    # Calculate Eye aspect ratio \n",
    "    left_ear,  _ = get_ear(landmarks, left_eye_idxs,  imgW, imgH)\n",
    "    right_ear, _ = get_ear(landmarks, right_eye_idxs, imgW, imgH)\n",
    "\n",
    "    Avg_EAR = (left_ear + right_ear) / 2\n",
    "    return Avg_EAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th code displayed below is designed to calculate the Eye Aspect Ratio (EAR) for two images: one with the eyes open (`test-open-eyes.jpg`) and one with the eyes closed (`test-close-eyes.jpg`). Here's a breakdown of what it does:\n",
    "\n",
    "1. **Image Loading**: It loads the two images using OpenCV's `imread` function and reverses the color channels from BGR to RGB.\n",
    "\n",
    "2. **Image Processing**: For each image, it ensures that the image data is stored in a contiguous block of memory and retrieves the image dimensions.\n",
    "\n",
    "3. **FaceMesh Model**: It initializes a MediaPipe FaceMesh model with `refine_landmarks=True` to improve landmark accuracy.\n",
    "\n",
    "4. **Landmark Detection**: It processes each image with the FaceMesh model to detect facial landmarks.\n",
    "\n",
    "5. **EAR Calculation**: If landmarks are detected, it calculates the average EAR for the left and right eyes using the `calculate_avg_ear` function (which is not shown in this snippet).\n",
    "\n",
    "6. **EAR Display**: It overlays the calculated EAR onto a copy of the original image.\n",
    "\n",
    "7. **Visualization**: It calls the `plot` function to visualize the facial landmarks on a copy of the original image.\n",
    "\n",
    "This code could be used in applications like drowsiness detection, where a decrease in EAR can indicate that a person is starting to fall asleep. Please note that this code assumes that functions like `calculate_avg_ear` and `plot`, as well as variables like `chosen_left_eye_idxs` and `chosen_right_eye_idxs`, are defined elsewhere in your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T17:23:27.728798Z",
     "start_time": "2022-09-22T17:23:26.966801Z"
    }
   },
   "outputs": [],
   "source": [
    "image_eyes_open  = cv2.imread(\"test-open-eyes.jpg\")[:, :, ::-1]\n",
    "image_eyes_close = cv2.imread(\"test-close-eyes.jpg\")[:, :, ::-1]\n",
    "\n",
    "for idx, image in enumerate([image_eyes_open, image_eyes_close]):\n",
    "    \n",
    "    image = np.ascontiguousarray(image)\n",
    "    imgH, imgW, _ = image.shape\n",
    "\n",
    "    # Creating a copy of the original image for plotting the EAR value\n",
    "    custom_chosen_lmk_image = image.copy()\n",
    "\n",
    "    # Running inference using static_image_mode\n",
    "    with mp_facemesh.FaceMesh(refine_landmarks=True) as face_mesh:\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        # If detections are available.\n",
    "        if results.multi_face_landmarks:\n",
    "\n",
    "            # Iterate over detections of each face. Here, we have max_num_faces=1, so only one iteration is performed.\n",
    "            for face_id, face_landmarks in enumerate(results.multi_face_landmarks):\n",
    "\n",
    "                landmarks = face_landmarks.landmark\n",
    "                EAR = calculate_avg_ear(landmarks, chosen_left_eye_idxs, chosen_right_eye_idxs, imgW, imgH)\n",
    "\n",
    "                # Print the EAR value on the custom_chosen_lmk_image.\n",
    "                cv2.putText(custom_chosen_lmk_image, f\"EAR: {round(EAR, 2)}\", (1, 24), \n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.9, (255, 255, 255), 2\n",
    "                )\n",
    "\n",
    "                plot(img_dt=image.copy(),img_eye_lmks_chosen=custom_chosen_lmk_image, face_landmarks=face_landmarks,\n",
    "                     ts_thickness=1, ts_circle_radius=3, lmk_circle_radius=3\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "75aa9c6daad11fbe7704e5dc5bd12ef893ea958e6c5f653ac1e3d84cdd3c4b71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
